{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2793ff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2890848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_json(url, file_name):\n",
    "    try:\n",
    "        res = requests.get(url, timeout=5)\n",
    "        data = res.json()\n",
    "\n",
    "        with open(f'../data/raw/{file_name}', 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "        \n",
    "        print(f'Successfully saved JSON data: {file_name}')\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0c019614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved JSON data: sf_tracts_new.geojson\n",
      "Successfully saved JSON data: sf_population_new.json\n",
      "Successfully saved JSON data: sf_water.geojson\n",
      "Successfully saved JSON data: bay_area_county_polygons.geojson\n",
      "Successfully saved JSON data: sfmta_routes.geojson\n",
      "Successfully saved JSON data: vehicle_data.json\n"
     ]
    }
   ],
   "source": [
    "fetch_json('https://api.censusreporter.org/1.0/geo/show/latest?geo_ids=140|05000US06075&format=geojson', 'sf_tracts_new.geojson')\n",
    "fetch_json('https://api.censusreporter.org/1.0/data/show/latest?table_ids=B01003&geo_ids=140|05000US06075', 'sf_population_new.json')\n",
    "fetch_json('https://data.sfgov.org/resource/xgse-mjer.geojson', 'sf_water.geojson')\n",
    "fetch_json('https://data.sfgov.org/resource/wamw-vt4s.geojson', 'bay_area_county_polygons.geojson')\n",
    "fetch_json('https://data.sfgov.org/resource/9exe-acju.geojson', 'sfmta_routes.geojson')\n",
    "fetch_json('https://api.censusreporter.org/1.0/data/show/latest?table_ids=B08201&geo_ids=140|05000US06075', 'vehicle_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d079e0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing...\n",
      "Loaded population JSON.\n",
      "Parsed population data into a DataFrame.\n",
      "Loaded census tract GeoJSON.\n",
      "Successfully merged population data with GeoJSON.\n",
      "Calculated area (sq miles) and population density (per sq mile).\n",
      "Projected final data back to WGS84.\n",
      "\n",
      "--- SUCCESS ---\n",
      "Successfully created final file: ../data/sf_tracts_with_density.geojson\n",
      "\n",
      "Preview of final data:\n",
      "                geoid                                    name  population  \\\n",
      "0  14000US06075010101  Census Tract 101.01, San Francisco, CA      2004.0   \n",
      "1  14000US06075010102  Census Tract 101.02, San Francisco, CA      1795.0   \n",
      "2  14000US06075010201  Census Tract 102.01, San Francisco, CA      2608.0   \n",
      "3  14000US06075010202  Census Tract 102.02, San Francisco, CA      1761.0   \n",
      "4  14000US06075010300     Census Tract 103, San Francisco, CA      3791.0   \n",
      "\n",
      "   area_sq_miles  population_density_sq_mi  \n",
      "0       0.518960               3861.572540  \n",
      "1       0.030726              58420.042081  \n",
      "2       0.072147              36148.254500  \n",
      "3       0.242356               7266.161679  \n",
      "4       0.103607              36590.249133  \n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define File Names ---\n",
    "population_json_file = \"../data/raw/sf_population.json\"\n",
    "tract_geojson_file = \"../data/raw/sf_tracts.geojson\"\n",
    "output_geojson_file = \"../data/sf_tracts_with_density.geojson\" \n",
    "\n",
    "print(\"Starting processing...\")\n",
    "\n",
    "try:\n",
    "    # --- 2. Load Population Data (JSON) ---\n",
    "    with open(population_json_file, 'r') as f:\n",
    "        pop_data = json.load(f)\n",
    "    print(\"Loaded population JSON.\")\n",
    "\n",
    "    # --- 3. Parse the Population JSON ---\n",
    "    POPULATION_KEY = 'B01003001'\n",
    "    population_list = []\n",
    "    \n",
    "    for geoid, tract_data in pop_data['data'].items():\n",
    "        try:\n",
    "            population = tract_data['B01003']['estimate'][POPULATION_KEY]\n",
    "            population_list.append({\n",
    "                'geoid': geoid,\n",
    "                'population': population\n",
    "            })\n",
    "        except KeyError:\n",
    "            print(f\"Warning: Could not find population data for geoid {geoid}\")\n",
    "\n",
    "    pop_df = pd.DataFrame(population_list)\n",
    "    print(\"Parsed population data into a DataFrame.\")\n",
    "\n",
    "    # --- 4. Load Geographic Data (GeoJSON) ---\n",
    "    gdf_tracts = gpd.read_file(tract_geojson_file)\n",
    "    print(\"Loaded census tract GeoJSON.\")\n",
    "\n",
    "    # --- 5. Merge Population and Geographic Data ---\n",
    "    merged_gdf = gdf_tracts.merge(pop_df, on='geoid')\n",
    "    print(\"Successfully merged population data with GeoJSON.\")\n",
    "\n",
    "    # --- 6. Project, Calculate Area, and Calculate Density (IN SQ MILES) ---\n",
    "    projected_gdf = merged_gdf.to_crs(epsg=3310) # Project to meters\n",
    "    projected_gdf['area_sq_meters'] = projected_gdf.geometry.area\n",
    "    \n",
    "    # --- UPDATED CONVERSION ---\n",
    "    # 1 square kilometer = 0.386102 square miles\n",
    "    projected_gdf['area_sq_km'] = projected_gdf['area_sq_meters'] / 1_000_000\n",
    "    projected_gdf['area_sq_miles'] = projected_gdf['area_sq_km'] * 0.386102\n",
    "    \n",
    "    # --- UPDATED DENSITY CALCULATION ---\n",
    "    projected_gdf['population_density_sq_mi'] = projected_gdf.apply(\n",
    "        lambda row: row['population'] / row['area_sq_miles'] if row['area_sq_miles'] > 0 else 0,\n",
    "        axis=1\n",
    "    )\n",
    "    print(\"Calculated area (sq miles) and population density (per sq mile).\")\n",
    "\n",
    "    # --- 7. Project back to WGS84 (EPSG:4326) for D3 ---\n",
    "    final_gdf = projected_gdf.to_crs(epsg=4326)\n",
    "    print(\"Projected final data back to WGS84.\")\n",
    "\n",
    "    # --- 8. Save the Final File (with new columns) ---\n",
    "    # --- UPDATED COLUMNS ---\n",
    "    columns_to_keep = [\n",
    "        'geometry',\n",
    "        'geoid',\n",
    "        'name',\n",
    "        'population',\n",
    "        'area_sq_miles', \n",
    "        'population_density_sq_mi' \n",
    "    ]\n",
    "    \n",
    "    final_gdf_clean = final_gdf[columns_to_keep]\n",
    "    final_gdf_clean.to_file(output_geojson_file, driver='GeoJSON')\n",
    "\n",
    "    print(\"\\n--- SUCCESS ---\")\n",
    "    print(f\"Successfully created final file: {output_geojson_file}\")\n",
    "    \n",
    "    print(\"\\nPreview of final data:\")\n",
    "    print(final_gdf_clean.drop(columns='geometry').head())\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: File not found. Make sure this file is in the same folder: {e.filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa333ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Bay Area file: ../data/bay_area_county_polygons.geojson...\n",
      "Found 'San Francisco' feature!\n",
      "\n",
      "--- SUCCESS ---\n",
      "Successfully created '../data/sf_county_boundary.geojson'\n",
      "This file is now ready to be used as your map's clipping mask.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "bay_area_file_name = \"../data/raw/bay_area_county_polygons.geojson\"\n",
    "output_file_name = \"../data/sf_county_boundary.geojson\"\n",
    "# --- End Configuration ---\n",
    "\n",
    "print(f\"Loading Bay Area file: {bay_area_file_name}...\")\n",
    "\n",
    "try:\n",
    "    # 1. Load the big Bay Area file\n",
    "    with open(bay_area_file_name, 'r') as f:\n",
    "        bay_area_data = json.load(f)\n",
    "\n",
    "    # 2. Find the San Francisco feature\n",
    "    sf_feature = None\n",
    "    for feature in bay_area_data['features']:\n",
    "        if feature.get('properties', {}).get('county') == 'San Francisco':\n",
    "            sf_feature = feature\n",
    "            print(\"Found 'San Francisco' feature!\")\n",
    "            break\n",
    "\n",
    "    if sf_feature:\n",
    "        # 3. Create a new, empty GeoJSON structure\n",
    "        # We copy the 'crs' (Coordinate Reference System) from the original\n",
    "        output_geojson = {\n",
    "            \"type\": \"FeatureCollection\",\n",
    "            \"crs\": bay_area_data.get('crs'), \n",
    "            \"features\": [sf_feature] # Add only the SF feature\n",
    "        }\n",
    "\n",
    "        # 5. Save the new, smaller file\n",
    "        with open(output_file_name, 'w') as f:\n",
    "            json.dump(output_geojson, f)\n",
    "            \n",
    "        print(f\"\\n--- SUCCESS ---\")\n",
    "        print(f\"Successfully created '{output_file_name}'\")\n",
    "        print(\"This file is now ready to be used as your map's clipping mask.\")\n",
    "\n",
    "    else:\n",
    "        print(\"ERROR: Could not find a feature with 'county': 'San Francisco'\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Could not find the file '{bay_area_file_name}'.\")\n",
    "    print(\"Please make sure it's in the same directory as this script.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "187a08c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../data/raw/sfmta_routes.geojson...\n",
      "File loaded. Searching for Geary routes...\n",
      "Found 4 features for routes: ['38', '38R']\n",
      "\n",
      "--- SUCCESS ---\n",
      "Successfully created '../data\\geary_route.geojson'\n",
      "This file contains the geometry for the 38 and 38R routes.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "routes_file = \"../data/raw/sfmta_routes.geojson\"\n",
    "output_file = \"geary_route.geojson\"\n",
    "output_folder = \"../data\" # Assumes you want to save it in your data folder\n",
    "# We'll focus on the main local and rapid routes\n",
    "routes_to_extract = ['38', '38R'] \n",
    "# --- End Configuration ---\n",
    "\n",
    "print(f\"Loading {routes_file}...\")\n",
    "\n",
    "try:\n",
    "    # Load the original routes file\n",
    "    with open(routes_file, 'r') as f:\n",
    "        routes_data = json.load(f)\n",
    "\n",
    "    print(\"File loaded. Searching for Geary routes...\")\n",
    "\n",
    "    # Create a list to hold the features we want to keep\n",
    "    geary_features = []\n",
    "    \n",
    "    # Check if 'features' key exists and is a list\n",
    "    if 'features' in routes_data and isinstance(routes_data['features'], list):\n",
    "        # Loop through all features\n",
    "        for feature in routes_data['features']:\n",
    "            if 'properties' in feature:\n",
    "                props = feature['properties']\n",
    "                \n",
    "                # Check if the 'route_name' is one we want\n",
    "                if props.get('route_name') in routes_to_extract:\n",
    "                    geary_features.append(feature)\n",
    "\n",
    "    if geary_features:\n",
    "        print(f\"Found {len(geary_features)} features for routes: {routes_to_extract}\")\n",
    "        \n",
    "        # Create a new, empty GeoJSON structure\n",
    "        # We copy the 'crs' (Coordinate Reference System) from the original\n",
    "        output_geojson = {\n",
    "            \"type\": \"FeatureCollection\",\n",
    "            \"crs\": routes_data.get('crs'), \n",
    "            \"features\": geary_features # Add only the Geary features\n",
    "        }\n",
    "\n",
    "        # 4. Make sure the 'data' folder exists\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "            print(f\"Created '{output_folder}' directory.\")\n",
    "\n",
    "        # 5. Save the new, smaller file\n",
    "        output_path = os.path.join(output_folder, output_file)\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(output_geojson, f)\n",
    "            \n",
    "        print(f\"\\n--- SUCCESS ---\")\n",
    "        print(f\"Successfully created '{output_path}'\")\n",
    "        print(\"This file contains the geometry for the 38 and 38R routes.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"ERROR: Could not find any features for routes: {routes_to_extract}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Could not find the file '{routes_file}'.\")\n",
    "    print(\"Please make sure it's in the same directory as this script.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c17dc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../data/sfmta_routes.geojson...\n",
      "File loaded. Searching for Light Rail routes: ['J', 'K', 'L', 'M', 'N', 'T']\n",
      "Found 12 features for rail routes.\n",
      "\n",
      "--- SUCCESS ---\n",
      "Successfully created '../data\\sfmta_rail_lines.geojson'\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "routes_file = \"../data/raw/sfmta_routes.geojson\"\n",
    "output_file = \"sfmta_rail_lines.geojson\"\n",
    "output_folder = \"../data\"\n",
    "# These are the light rail lines\n",
    "routes_to_extract = ['J', 'K', 'L', 'M', 'N', 'T']\n",
    "# --- End Configuration ---\n",
    "\n",
    "print(f\"Loading {routes_file}...\")\n",
    "\n",
    "try:\n",
    "    with open(routes_file, 'r') as f:\n",
    "        routes_data = json.load(f)\n",
    "\n",
    "    print(f\"File loaded. Searching for Light Rail routes: {routes_to_extract}\")\n",
    "\n",
    "    rail_features = []\n",
    "    \n",
    "    if 'features' in routes_data and isinstance(routes_data['features'], list):\n",
    "        for feature in routes_data['features']:\n",
    "            if 'properties' in feature:\n",
    "                props = feature['properties']\n",
    "                if props.get('route_name') in routes_to_extract:\n",
    "                    rail_features.append(feature)\n",
    "\n",
    "    if rail_features:\n",
    "        print(f\"Found {len(rail_features)} features for rail routes.\")\n",
    "        \n",
    "        output_geojson = {\n",
    "            \"type\": \"FeatureCollection\",\n",
    "            \"crs\": routes_data.get('crs'), \n",
    "            \"features\": rail_features\n",
    "        }\n",
    "\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "            print(f\"Created '{output_folder}' directory.\")\n",
    "\n",
    "        output_path = os.path.join(output_folder, output_file)\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(output_geojson, f)\n",
    "            \n",
    "        print(f\"\\n--- SUCCESS ---\")\n",
    "        print(f\"Successfully created '{output_path}'\")\n",
    "\n",
    "    else:\n",
    "        print(f\"ERROR: Could not find any rail line features.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Could not find the file '{routes_file}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68377285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BART stations file: ../data/raw/bart_stations.geojson...\n",
      "Found 10 stations (SF + Daly City + West Oakland).\n",
      "\n",
      "--- SUCCESS ---\n",
      "Successfully created '../data\\sf_bart_stations.geojson'\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "bart_stations_file = \"../data/raw/bart_stations.geojson\"\n",
    "output_file_name = \"sf_bart_stations.geojson\"\n",
    "output_folder = \"../data\"\n",
    "\n",
    "# Stations to find\n",
    "# We use 'Name' for the non-SF ones just to be safe\n",
    "stations_to_keep = {\n",
    "    'City': ['San Francisco'],\n",
    "    'Name': ['Daly City', 'West Oakland']\n",
    "}\n",
    "# --- End Configuration ---\n",
    "\n",
    "print(f\"Loading BART stations file: {bart_stations_file}...\")\n",
    "\n",
    "try:\n",
    "    with open(bart_stations_file, 'r') as f:\n",
    "        bart_data = json.load(f)\n",
    "\n",
    "    sf_station_features = []\n",
    "    \n",
    "    if 'features' in bart_data and isinstance(bart_data['features'], list):\n",
    "        for feature in bart_data['features']:\n",
    "            props = feature.get('properties', {})\n",
    "            \n",
    "            # Check if it's an SF station OR one of our named stations\n",
    "            if (props.get('City') in stations_to_keep['City'] or \n",
    "                props.get('Name') in stations_to_keep['Name']):\n",
    "                \n",
    "                sf_station_features.append(feature)\n",
    "    \n",
    "    if sf_station_features:\n",
    "        print(f\"Found {len(sf_station_features)} stations (SF + Daly City + West Oakland).\")\n",
    "        \n",
    "        output_geojson = {\n",
    "            \"type\": \"FeatureCollection\",\n",
    "            \"crs\": bart_data.get('crs'), \n",
    "            \"features\": sf_station_features\n",
    "        }\n",
    "\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "\n",
    "        output_path = os.path.join(output_folder, output_file_name)\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(output_geojson, f)\n",
    "            \n",
    "        print(f\"\\n--- SUCCESS ---\")\n",
    "        print(f\"Successfully created '{output_path}'\")\n",
    "\n",
    "    else:\n",
    "        print(\"ERROR: Could not find any of the target stations.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Could not find the file '{bart_stations_file}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e7e9b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../data/raw/sfmta_ridership.csv...\n",
      "Using data from the most recent full year: 2024\n",
      "\n",
      "--- Top 10 Corridors by Total Weekday Ridership (2024) ---\n",
      "           Corridor  Bus_Ridership  BART_Ridership  Total_Ridership       Highlight\n",
      "   Mission Corridor   46316.666667           12690     59006.666667 Other Corridors\n",
      "49 Van Ness/Mission   33925.000000           12690     46615.000000 Other Corridors\n",
      "     Geary Corridor   41808.333333               0     41808.333333  Geary Corridor\n",
      "        22 Fillmore   21783.333333               0     21783.333333 Other Corridors\n",
      "         8 Bayshore   17758.333333               0     17758.333333 Other Corridors\n",
      "       1 California   17408.333333               0     17408.333333 Other Corridors\n",
      "        30 Stockton   16391.666667               0     16391.666667 Other Corridors\n",
      "          29 Sunset   15400.000000               0     15400.000000 Other Corridors\n",
      "   44 O'Shaughnessy   12241.666667               0     12241.666667 Other Corridors\n",
      "   7 Haight/Noriega   10958.333333               0     10958.333333 Other Corridors\n",
      "\n",
      "Successfully created summary JSON file: ../data\\corridor_ridership_summary.json\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define File Names ---\n",
    "ridership_file = \"../data/raw/sfmta_ridership.csv\"\n",
    "output_file = \"corridor_ridership_summary.json\"\n",
    "output_folder = \"../data\"\n",
    "\n",
    "print(f\"Loading {ridership_file}...\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(ridership_file)\n",
    "\n",
    "    # --- 2. Clean Data ---\n",
    "    # Convert 'Average Daily Boardings' to a clean number\n",
    "    df['Average Daily Boardings'] = df['Average Daily Boardings'].str.replace(',', '')\n",
    "    df['Average Daily Boardings'] = pd.to_numeric(df['Average Daily Boardings'], errors='coerce')\n",
    "    df.dropna(subset=['Average Daily Boardings'], inplace=True)\n",
    "\n",
    "    # Convert 'Month' to datetime to find the year\n",
    "    df['Month'] = pd.to_datetime(df['Month'], format='%B %Y')\n",
    "    df['Year'] = df['Month'].dt.year\n",
    "    \n",
    "    # Use 2024 as the most recent full year\n",
    "    target_year = 2024\n",
    "    print(f\"Using data from the most recent full year: {target_year}\")\n",
    "    \n",
    "    # Filter for target year, weekdays, and only bus routes\n",
    "    non_bus_categories = ['Muni Metro', 'Cable Car', 'Historic Streetcar']\n",
    "    bus_df = df[\n",
    "        (df['Year'] == target_year) &\n",
    "        (df['Service Day of the Week'] == 'Weekday') &\n",
    "        (~df['Service Category'].isin(non_bus_categories))\n",
    "    ].copy()\n",
    "\n",
    "    # --- 3. Aggregate into Corridors ---\n",
    "    # Get the yearly average for each route\n",
    "    avg_annual_ridership = bus_df.groupby('Route')['Average Daily Boardings'].mean().reset_index()\n",
    "    \n",
    "    # Define corridor groups\n",
    "    corridor_map = {\n",
    "        '38 Geary': 'Geary Corridor',\n",
    "        '38R Geary Rapid': 'Geary Corridor',\n",
    "        '14 Mission': 'Mission Corridor',\n",
    "        '14R Mission Rapid': 'Mission Corridor',\n",
    "    }\n",
    "    \n",
    "    # Map routes to corridors, fill unmapped routes with their own name\n",
    "    avg_annual_ridership['Corridor'] = avg_annual_ridership['Route'].map(corridor_map).fillna(avg_annual_ridership['Route'])\n",
    "    \n",
    "    # Group by the new 'Corridor' and sum the boardings\n",
    "    corridor_totals = avg_annual_ridership.groupby('Corridor')['Average Daily Boardings'].sum().reset_index()\n",
    "    corridor_totals.rename(columns={'Average Daily Boardings': 'Bus_Ridership'}, inplace=True)\n",
    "\n",
    "    # --- 4. Manually Add BART Ridership Data ---\n",
    "    # Based on our analysis of BART data (16th St + 24th St)\n",
    "    BART_MISSION_RIDERSHIP = 12690\n",
    "    \n",
    "    corridor_totals['BART_Ridership'] = 0\n",
    "    \n",
    "    # Find the index for Mission Corridor and add BART ridership\n",
    "    mission_index = corridor_totals.index[corridor_totals['Corridor'] == 'Mission Corridor']\n",
    "    if not mission_index.empty:\n",
    "        corridor_totals.at[mission_index[0], 'BART_Ridership'] = BART_MISSION_RIDERSHIP\n",
    "\n",
    "    van_ness_index = corridor_totals.index[corridor_totals['Corridor'] == '49 Van Ness/Mission']\n",
    "    if not van_ness_index.empty:\n",
    "        corridor_totals.at[van_ness_index[0], 'BART_Ridership'] = BART_MISSION_RIDERSHIP\n",
    "    \n",
    "    # --- 5. Final Calculations and Sorting ---\n",
    "    corridor_totals['Total_Ridership'] = corridor_totals['Bus_Ridership'] + corridor_totals['BART_Ridership']\n",
    "    \n",
    "    # Get the Top 10 Corridors\n",
    "    top_10_corridors = corridor_totals.sort_values(by='Total_Ridership', ascending=False).head(10).copy()\n",
    "    \n",
    "    # Add a highlight column for D3\n",
    "    top_10_corridors['Highlight'] = top_10_corridors['Corridor'].apply(\n",
    "        lambda x: 'Geary Corridor' if 'Geary' in x else 'Other Corridors'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n--- Top 10 Corridors by Total Weekday Ridership (2024) ---\")\n",
    "    print(top_10_corridors.to_string(index=False))\n",
    "\n",
    "    # --- 6. Save to JSON for D3 ---\n",
    "    # Convert DataFrame to a list of dictionaries (records)\n",
    "    output_data = top_10_corridors.to_dict(orient='records')\n",
    "    \n",
    "    # Make sure 'data' folder exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        print(f\"Created '{output_folder}' directory.\")\n",
    "        \n",
    "    output_path = os.path.join(output_folder, output_file)\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(output_data, f, indent=2)\n",
    "        \n",
    "    print(f\"\\nSuccessfully created summary JSON file: {output_path}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Could not find the file '{ridership_file}'.\")\n",
    "    print(\"Please make sure it's in the same directory as this script.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a5a063d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../data/raw/sfmta_ridership.csv...\n",
      "\n",
      "--- Processed Corridor Growth Data (Sample) ---\n",
      "[\n",
      "  {\n",
      "    \"corridor\": \"1 California\",\n",
      "    \"values\": [\n",
      "      {\n",
      "        \"year\": 2019,\n",
      "        \"ridership\": 22716\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2020,\n",
      "        \"ridership\": 9375\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2021,\n",
      "        \"ridership\": 10100\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2022,\n",
      "        \"ridership\": 13783\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2023,\n",
      "        \"ridership\": 15816\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2024,\n",
      "        \"ridership\": 17408\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2025,\n",
      "        \"ridership\": 18620\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"corridor\": \"22 Fillmore\",\n",
      "    \"values\": [\n",
      "      {\n",
      "        \"year\": 2019,\n",
      "        \"ridership\": 17758\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2020,\n",
      "        \"ridership\": 10158\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2021,\n",
      "        \"ridership\": 11775\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2022,\n",
      "        \"ridership\": 16741\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2023,\n",
      "        \"ridership\": 19625\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2024,\n",
      "        \"ridership\": 21783\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2025,\n",
      "        \"ridership\": 22490\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"corridor\": \"49 Van Ness/Mission\",\n",
      "    \"values\": [\n",
      "      {\n",
      "        \"year\": 2019,\n",
      "        \"ridership\": 24450\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2020,\n",
      "        \"ridership\": 14183\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2021,\n",
      "        \"ridership\": 15266\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2022,\n",
      "        \"ridership\": 23791\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2023,\n",
      "        \"ridership\": 30991\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2024,\n",
      "        \"ridership\": 33925\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2025,\n",
      "        \"ridership\": 35040\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"corridor\": \"8 Bayshore\",\n",
      "    \"values\": [\n",
      "      {\n",
      "        \"year\": 2019,\n",
      "        \"ridership\": 22591\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2020,\n",
      "        \"ridership\": 14308\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2021,\n",
      "        \"ridership\": 14766\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2022,\n",
      "        \"ridership\": 17325\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2023,\n",
      "        \"ridership\": 16950\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2024,\n",
      "        \"ridership\": 17758\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2025,\n",
      "        \"ridership\": 18240\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"corridor\": \"Geary Corridor\",\n",
      "    \"values\": [\n",
      "      {\n",
      "        \"year\": 2019,\n",
      "        \"ridership\": 53308\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2020,\n",
      "        \"ridership\": 24541\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2021,\n",
      "        \"ridership\": 24283\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2022,\n",
      "        \"ridership\": 32375\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2023,\n",
      "        \"ridership\": 37966\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2024,\n",
      "        \"ridership\": 41808\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2025,\n",
      "        \"ridership\": 44250\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"corridor\": \"Mission Corridor\",\n",
      "    \"values\": [\n",
      "      {\n",
      "        \"year\": 2019,\n",
      "        \"ridership\": 48233\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2020,\n",
      "        \"ridership\": 29533\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2021,\n",
      "        \"ridership\": 33575\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2022,\n",
      "        \"ridership\": 37875\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2023,\n",
      "        \"ridership\": 44075\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2024,\n",
      "        \"ridership\": 46316\n",
      "      },\n",
      "      {\n",
      "        \"year\": 2025,\n",
      "        \"ridership\": 46230\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "\n",
      "Successfully created summary JSON file: ../data\\corridor_growth_data.json\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define File Names ---\n",
    "ridership_file = \"../data/raw/sfmta_ridership.csv\"\n",
    "output_file = \"corridor_growth_data.json\"\n",
    "output_folder = \"../data\"\n",
    "\n",
    "print(f\"Loading {ridership_file}...\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(ridership_file)\n",
    "\n",
    "    # --- 2. Clean Data ---\n",
    "    df['Average Daily Boardings'] = df['Average Daily Boardings'].astype(str).str.replace(',', '')\n",
    "    df['Average Daily Boardings'] = pd.to_numeric(df['Average Daily Boardings'], errors='coerce')\n",
    "    df.dropna(subset=['Average Daily Boardings'], inplace=True)\n",
    "    df['Average Daily Boardings'] = df['Average Daily Boardings'].astype(int)\n",
    "\n",
    "    df['Month'] = pd.to_datetime(df['Month'], format='%B %Y')\n",
    "    df['Year'] = df['Month'].dt.year\n",
    "    \n",
    "    # Filter for Weekday and Bus Routes only\n",
    "    non_bus_categories = ['Muni Metro', 'Cable Car', 'Historic Streetcar']\n",
    "    bus_df = df[\n",
    "        (df['Service Day of the Week'] == 'Weekday') &\n",
    "        (~df['Service Category'].isin(non_bus_categories))\n",
    "    ].copy()\n",
    "\n",
    "    # --- 3. Aggregate into Corridors ---\n",
    "    \n",
    "    # Define corridor groups\n",
    "    # We now group 14, 14R, and 49 into the Mission Corridor\n",
    "    corridor_map = {\n",
    "        '38 Geary': 'Geary Corridor',\n",
    "        '38R Geary Rapid': 'Geary Corridor',\n",
    "        '14 Mission': 'Mission Corridor',\n",
    "        '14R Mission Rapid': 'Mission Corridor',\n",
    "        '49 Van Ness/Mission': '49 Van Ness/Mission',\n",
    "        '22 Fillmore': '22 Fillmore',\n",
    "        '8 Bayshore': '8 Bayshore',\n",
    "        '1 California': '1 California',\n",
    "    }\n",
    "    \n",
    "    # Map routes to corridors\n",
    "    bus_df['Corridor'] = bus_df['Route'].map(corridor_map)\n",
    "    \n",
    "    # Filter out routes that are not in our main corridors\n",
    "    main_corridors = [\n",
    "        'Geary Corridor', \n",
    "        'Mission Corridor',\n",
    "        '49 Van Ness/Mission',\n",
    "        '22 Fillmore', \n",
    "        '8 Bayshore', \n",
    "        '1 California',\n",
    "    ]\n",
    "    bus_df = bus_df[bus_df['Corridor'].isin(main_corridors)]\n",
    "\n",
    "    # --- 4. Group by Year and Corridor ---\n",
    "    # We must calculate the *annual average* for each route first,\n",
    "    # then sum those averages into corridors.\n",
    "    \n",
    "    # Get the average for each route for each year\n",
    "    annual_avg_route = bus_df.groupby(['Year', 'Route'])['Average Daily Boardings'].mean().reset_index()\n",
    "    \n",
    "    # Map the routes to corridors\n",
    "    annual_avg_route['Corridor'] = annual_avg_route['Route'].map(corridor_map)\n",
    "    \n",
    "    # Finally, sum the averages by Year and Corridor\n",
    "    corridor_growth = annual_avg_route.groupby(['Year', 'Corridor'])['Average Daily Boardings'].sum().reset_index()\n",
    "    \n",
    "    # --- 5. Format Data for D3 (Nesting) ---\n",
    "    # This creates the [ {corridor: \"Name\", values: [ {year: Y, ridership: R}, ... ]}, ... ] structure\n",
    "    \n",
    "    d3_data = []\n",
    "    \n",
    "    for corridor_name in corridor_growth['Corridor'].unique():\n",
    "        corridor_data = corridor_growth[corridor_growth['Corridor'] == corridor_name]\n",
    "        \n",
    "        values_list = []\n",
    "        for index, row in corridor_data.iterrows():\n",
    "            values_list.append({\n",
    "                \"year\": int(row['Year']),\n",
    "                \"ridership\": int(row['Average Daily Boardings'])\n",
    "            })\n",
    "        \n",
    "        d3_data.append({\n",
    "            \"corridor\": corridor_name,\n",
    "            \"values\": values_list\n",
    "        })\n",
    "        \n",
    "    print(\"\\n--- Processed Corridor Growth Data (Sample) ---\")\n",
    "    print(json.dumps(d3_data, indent=2))\n",
    "\n",
    "    # --- 6. Save to JSON for D3 ---\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        print(f\"Created '{output_folder}' directory.\")\n",
    "        \n",
    "    output_path = os.path.join(output_folder, output_file)\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(d3_data, f, indent=2)\n",
    "        \n",
    "    print(f\"\\nSuccessfully created summary JSON file: {output_path}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Could not find the file '{ridership_file}'.\")\n",
    "    print(\"Please make sure it's in the same directory as this script.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8e3736af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to merge equity data...\n",
      "Loaded vehicle_data.json\n",
      "Loaded base map file (sf_tracts_with_density.geojson)\n",
      "Successfully merged density data and equity data.\n",
      "\n",
      "--- SUCCESS ---\n",
      "Successfully created final file: ../data/sf_map_data.json\n",
      "This file contains all data (density, equity, etc.)\n",
      "\n",
      "Preview of final data properties:\n",
      "                geoid                                    name  population  \\\n",
      "0  14000US06075010101  Census Tract 101.01, San Francisco, CA      2004.0   \n",
      "1  14000US06075010102  Census Tract 101.02, San Francisco, CA      1795.0   \n",
      "2  14000US06075010201  Census Tract 102.01, San Francisco, CA      2608.0   \n",
      "3  14000US06075010202  Census Tract 102.02, San Francisco, CA      1761.0   \n",
      "4  14000US06075010300     Census Tract 103, San Francisco, CA      3791.0   \n",
      "\n",
      "   area_sq_miles  population_density_sq_mi  total_households  no_vehicles  \\\n",
      "0       0.518960               3861.572540            1212.0        459.0   \n",
      "1       0.030726              58420.042081             907.0        303.0   \n",
      "2       0.072147              36148.254500            1375.0        452.0   \n",
      "3       0.242356               7266.161679            1091.0        206.0   \n",
      "4       0.103607              36590.249133            2143.0        500.0   \n",
      "\n",
      "   percent_no_vehicle  \n",
      "0           37.871287  \n",
      "1           33.406836  \n",
      "2           32.872727  \n",
      "3           18.881760  \n",
      "4           23.331778  \n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "# Your existing map file (the one with density)\n",
    "base_map_file = \"../data/sf_tracts_with_density.geojson\" \n",
    "# The new file you just downloaded\n",
    "vehicle_data_file = \"../data/raw/vehicle_data.json\"\n",
    "# The new file we will create\n",
    "output_file = \"../data/sf_map_data.json\" \n",
    "\n",
    "# Census table keys\n",
    "TOTAL_HOUSEHOLDS_KEY = 'B08201001'\n",
    "NO_VEHICLES_KEY = 'B08201002'\n",
    "# --- End Configuration ---\n",
    "\n",
    "print(\"Starting to merge equity data...\")\n",
    "\n",
    "try:\n",
    "    # 1. Load the new vehicle data JSON\n",
    "    with open(vehicle_data_file, 'r') as f:\n",
    "        vehicle_data = json.load(f)\n",
    "    print(\"Loaded vehicle_data.json\")\n",
    "\n",
    "    # 2. Parse the vehicle data into a list\n",
    "    equity_list = []\n",
    "    for geoid, tract_data in vehicle_data['data'].items():\n",
    "        try:\n",
    "            total_households = tract_data['B08201']['estimate'][TOTAL_HOUSEHOLDS_KEY]\n",
    "            no_vehicles = tract_data['B08201']['estimate'][NO_VEHICLES_KEY]\n",
    "            \n",
    "            # Calculate percentage\n",
    "            percent_no_vehicle = (no_vehicles / total_households) * 100 if total_households > 0 else 0\n",
    "            \n",
    "            equity_list.append({\n",
    "                'geoid': geoid,\n",
    "                'total_households': total_households,\n",
    "                'no_vehicles': no_vehicles,\n",
    "                'percent_no_vehicle': percent_no_vehicle\n",
    "            })\n",
    "        except KeyError:\n",
    "            print(f\"Warning: Could not find vehicle data for geoid {geoid}\")\n",
    "\n",
    "    # 3. Convert list to a DataFrame for merging\n",
    "    import pandas as pd\n",
    "    equity_df = pd.DataFrame(equity_list)\n",
    "    \n",
    "    # 4. Load your base GeoJSON file\n",
    "    gdf = gpd.read_file(base_map_file)\n",
    "    print(\"Loaded base map file (sf_tracts_with_density.geojson)\")\n",
    "    \n",
    "    # 5. Merge the new data\n",
    "    # The 'geoid' in our new data matches the 'geoid' in your map file\n",
    "    merged_gdf = gdf.merge(equity_df, on='geoid')\n",
    "    print(\"Successfully merged density data and equity data.\")\n",
    "\n",
    "    # 6. Save the new, final GeoJSON file\n",
    "    merged_gdf.to_file(output_file, driver='GeoJSON')\n",
    "    \n",
    "    print(f\"\\n--- SUCCESS ---\")\n",
    "    print(f\"Successfully created final file: {output_file}\")\n",
    "    print(\"This file contains all data (density, equity, etc.)\")\n",
    "    print(\"\\nPreview of final data properties:\")\n",
    "    print(merged_gdf.drop(columns='geometry').head())\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: File not found. Make sure this file is in your data folder: {e.filename}\")\n",
    "except ImportError:\n",
    "    print(\"\\n--- ERROR ---\")\n",
    "    print(\"You need 'geopandas' and 'pandas' for this script.\")\n",
    "    print(\"Please run: pip install pandas geopandas\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
